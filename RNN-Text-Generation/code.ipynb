{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torchmetrics.functional import accuracy\n",
    "import random\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique characters in the corpus is 65\n",
      "A slice of the unique characters set:\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    vocab = sorted(set(data))\n",
    "    print ('The number of unique characters in the corpus is', len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, seq_length=100):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.int_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.data_size, self.vocab_size = len(text), len(self.chars)\n",
    "        self.seq_length = seq_length\n",
    "        # self.embedding_dim = L.Embedding(num_embeddings=self.vocab_size, embedding_dim=256)\n",
    "\n",
    "        # Create training data\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for i in range(0, self.data_size - seq_length, 1):\n",
    "            seq_in = text[i:i + seq_length]\n",
    "            seq_out = text[i + seq_length]\n",
    "            self.x.append([self.char_to_int[char] for char in seq_in])\n",
    "            self.y.append(self.char_to_int[seq_out])\n",
    "        self.x = np.array(self.x)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index], dtype=torch.long), torch.tensor(self.y[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, seq_length=100, train_split=0.7, valid_split=0.15):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    dataset = ShakespeareDataset(text, seq_length)\n",
    "    train_size = int(len(dataset) * train_split)\n",
    "    valid_size = int(len(dataset) * valid_split)\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, layers):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss, logger=True, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss, logger=True, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 8.3 K \n",
      "1 | lstm      | LSTM      | 921 K \n",
      "2 | fc        | Linear    | 16.7 K\n",
      "----------------------------------------\n",
      "946 K     Trainable params\n",
      "0         Non-trainable params\n",
      "946 K     Total params\n",
      "3.786     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbe4e525ea24cc3ae8f8cab0bf92fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef8333e4b994265997e152ac9860d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68848aede4fb43ae872c4af922ecf882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c28a6f7c884eef9f719843444025a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e585593470ef46ce8d841b2c0a1d6a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a859154df304625904c497abab2f98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5656c4bf03849b395950842cc3dbce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4839a6f5044f40d2b67d2e51d00cf5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2e74519c0841a78d1efe53c53b92b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c54bc2f59745b29a67accf5c174fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5916758ec83b48739464ae200fef498f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your datasets\n",
    "train_dataset, valid_dataset, test_dataset = load_data('input.txt', seq_length=100)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Create the model\n",
    "model = TextGenerator(len(train_dataset.dataset.chars), embedding_dim=128, hidden_dim=256, layers=2)\n",
    "\n",
    "# Setup trainer and fit the model using Lightning's Trainer\n",
    "trainer = L.Trainer(max_epochs=20, callbacks=[ModelCheckpoint(monitor=\"val_loss\")])\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Foul not to can do\n",
      "The sent to the dust\n",
      "To say the truth\n",
      "A said to the sent\n",
      "At our grant thou hate\n",
      "What he had would\n",
      "never not to the truth\n",
      "And stand so head\n",
      "The head it.\n",
      "Come, and legs,\n",
      "As it was not\n",
      "Betweed some hath\n",
      "a custom me to thee sure\n",
      "The faith, as we have heart\n",
      "With my son,\n",
      "I will not our return't\n",
      "Than thou still they we have before\n",
      "Another-in tribunes\n",
      "To say an every\n",
      "Was it is\n",
      "To the truth,\n",
      "For that I talk there is a\n",
      "fatting great man\n",
      "The consulded\n",
      "That stand catchious have more\n",
      "All thou speak\n",
      "That saying, or how it\n",
      "That before things\n",
      "In all the live\n",
      "With of the truth\n",
      "That stand cheeks\n",
      "That way their\n",
      "Than the first trum;\n",
      "And such as\n",
      "With the even thanks\n",
      "Which with all dreams\n",
      "That stand so it\n",
      "With his breaths\n",
      "at it was a\n",
      "still apt at\n",
      "Our strive of patiens\n",
      "There we will before\n",
      "That was a grant\n",
      "With the sun of your voices\n",
      "Stand and tell your with his\n",
      "but the gods.\n",
      "\n",
      "AUFIDIU stand they\n",
      "And still in this sure\n",
      "The conselly,\n",
      "If they are not\n",
      "By the people.\n",
      "\n",
      "SICINIUS: then, insurate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_str, gen_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    input_seq = [test_dataset.dataset.char_to_int[c] for c in start_str[-test_dataset.dataset.seq_length:]]\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(model.device)\n",
    "\n",
    "    text = start_str\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(gen_length):\n",
    "        output, hidden = model.lstm(model.embedding(input_seq), hidden)\n",
    "        output_logits = model.fc(output[:, -1, :])\n",
    "        p = torch.nn.functional.softmax(output_logits / temperature, dim=-1).detach().cpu().numpy().squeeze()\n",
    "        char_ind = np.random.choice(len(test_dataset.dataset.chars), p=p)\n",
    "        next_char = test_dataset.dataset.int_to_char[char_ind]\n",
    "        text += next_char\n",
    "\n",
    "        input_seq = torch.cat((input_seq[:, 1:], torch.tensor([[char_ind]], dtype=torch.long).to(model.device)), dim=1)\n",
    "\n",
    "    return text\n",
    "\n",
    "print(generate_text(model, 'ROMEO:', gen_length=1000, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics.functional import accuracy\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, seq_length=100):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.int_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.data_size, self.vocab_size = len(text), len(self.chars)\n",
    "        self.seq_length = seq_length\n",
    "        # self.embedding_dim = L.Embedding(num_embeddings=self.vocab_size, embedding_dim=256)\n",
    "\n",
    "        # Create training data\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for i in range(0, self.data_size - seq_length, 1):\n",
    "            seq_in = text[i:i + seq_length]\n",
    "            seq_out = text[i + seq_length]\n",
    "            self.x.append([self.char_to_int[char] for char in seq_in])\n",
    "            self.y.append(self.char_to_int[seq_out])\n",
    "        self.x = np.array(self.x)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index], dtype=torch.long), torch.tensor(self.y[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, seq_length=20):\n",
    "        # Preprocess text to handle special characters\n",
    "        # text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        words = text.split()\n",
    "        self.data_size, self.vocab_size = len(words), len(set(words))\n",
    "        self.words = sorted(list(set(words)))[:self.vocab_size-1] + ['<UNK>']\n",
    "        self.word_to_int = {w: i for i, w in enumerate(self.words)}\n",
    "        self.int_to_word = {i: w for i, w in enumerate(self.words)}\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        for i in range(0, self.data_size - seq_length, 1):\n",
    "            seq_in = words[i:i + seq_length]\n",
    "            seq_out = words[i + seq_length]\n",
    "            self.x.append([self.word_to_int.get(word, self.vocab_size-1) for word in seq_in])\n",
    "            self.y.append(self.word_to_int.get(seq_out, self.vocab_size-1))\n",
    "        self.x = np.array(self.x)\n",
    "        self.y = np.array(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.x[index], dtype=torch.long), torch.tensor(self.y[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, seq_length=100, train_split=0.7, valid_split=0.15):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    dataset = ShakespeareDataset(text, seq_length)\n",
    "    train_size = int(len(dataset) * train_split)\n",
    "    valid_size = int(len(dataset) * valid_split)\n",
    "    test_size = len(dataset) - train_size - valid_size\n",
    "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "    return dataset, train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, layers, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss, logger=True, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss, logger=True, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your datasets\n",
    "dataset, train_dataset, valid_dataset, test_dataset = load_data('input.txt', seq_length=100)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type      | Params\n",
      "----------------------------------------\n",
      "0 | embedding | Embedding | 8.3 K \n",
      "1 | lstm      | LSTM      | 921 K \n",
      "2 | fc        | Linear    | 16.7 K\n",
      "----------------------------------------\n",
      "946 K     Trainable params\n",
      "0         Non-trainable params\n",
      "946 K     Total params\n",
      "3.786     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97564000b694adfb46e863db7f3ed40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyy/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/yyy/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f72a0e03cf342c3bb584b07be7bd2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc6d12f3118451ba0087c645e177644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b27eb42d017412995792f2177016a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c304e544d78948ed9eca1c1892820473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d305dfe1694434c93e71c7caa8917b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6ca3027fb847198cb3f58021143000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyy/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = TextGenerator(len(train_dataset.dataset.chars), embedding_dim=128, hidden_dim=256, layers=2)\n",
    "\n",
    "# Setup trainer and fit the model using Lightning's Trainer\n",
    "trainer = L.Trainer(max_epochs=10, callbacks=[ModelCheckpoint(monitor=\"val_loss\"), EarlyStopping(monitor=\"val_loss\")])\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 torch.save(model, <span style=\"color: #808000; text-decoration-color: #808000\">'model10_10_epoch_1_layer_all_data.pth'</span>)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/yyy/Library/Python/3.9/lib/python/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">629</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">save</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 626 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 627 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> _use_new_zipfile_serialization:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 628 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> _open_zipfile_writer(f) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> opened_zipfile:                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 629 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>_save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorde  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 630 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 631 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 632 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> _open_file_like(f, <span style=\"color: #808000; text-decoration-color: #808000\">'wb'</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> opened_file:                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/yyy/Library/Python/3.9/lib/python/site-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">841</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_save</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 838 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>data_buf = io.BytesIO()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 839 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>pickler = pickle_module.Pickler(data_buf, protocol=pickle_protocol)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 840 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>pickler.persistent_id = persistent_id                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 841 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>pickler.dump(obj)                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 842 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>data_value = data_buf.getvalue()                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 843 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>zip_file.write_record(<span style=\"color: #808000; text-decoration-color: #808000\">'data.pkl'</span>, data_value, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(data_value))                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 844 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">PicklingError: </span>Can't pickle <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.TextGenerator'</span><span style=\"font-weight: bold\">&gt;</span>: it's not the same object as __main__.TextGenerator\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 torch.save(model, \u001b[33m'\u001b[0m\u001b[33mmodel10_10_epoch_1_layer_all_data.pth\u001b[0m\u001b[33m'\u001b[0m)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/yyy/Library/Python/3.9/lib/python/site-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m629\u001b[0m in \u001b[92msave\u001b[0m        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 626 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 627 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m _use_new_zipfile_serialization:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 628 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m _open_zipfile_writer(f) \u001b[94mas\u001b[0m opened_zipfile:                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 629 \u001b[2m│   │   │   \u001b[0m_save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorde  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 630 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 631 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 632 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m _open_file_like(f, \u001b[33m'\u001b[0m\u001b[33mwb\u001b[0m\u001b[33m'\u001b[0m) \u001b[94mas\u001b[0m opened_file:                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/yyy/Library/Python/3.9/lib/python/site-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m841\u001b[0m in \u001b[92m_save\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 838 \u001b[0m\u001b[2m│   \u001b[0mdata_buf = io.BytesIO()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 839 \u001b[0m\u001b[2m│   \u001b[0mpickler = pickle_module.Pickler(data_buf, protocol=pickle_protocol)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 840 \u001b[0m\u001b[2m│   \u001b[0mpickler.persistent_id = persistent_id                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 841 \u001b[2m│   \u001b[0mpickler.dump(obj)                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 842 \u001b[0m\u001b[2m│   \u001b[0mdata_value = data_buf.getvalue()                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 843 \u001b[0m\u001b[2m│   \u001b[0mzip_file.write_record(\u001b[33m'\u001b[0m\u001b[33mdata.pkl\u001b[0m\u001b[33m'\u001b[0m, data_value, \u001b[96mlen\u001b[0m(data_value))                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 844 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mPicklingError: \u001b[0mCan't pickle \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'__main__.TextGenerator'\u001b[0m\u001b[1m>\u001b[0m: it's not the same object as __main__.TextGenerator\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model, 'model11_10_epoch_1_layer_word.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: now bless me stand to be to any of you but we are undone to hate it is a very man Aumerle the very name of his hands and all the Duke of Norfolk to the people and give his last to be a best than my heart Are he hath found us out of your death And look upon him and if the world be the duke will have some men to be a world As she is a kind of late And weakling I am no more of my fathers lord and a noble friend To Lord of my head to the king I will not be ruled with thee And that I did not call thee more than the king and fearful of a means And in the devil of the world I know twas to the king And make thee say I do not say I take thee for thy hand I am too long till thou art a guest of this intent That is your name of Edwards age Now hath I neer heard me to the king and I will not say by my life and my joy of this land And if I be known with you sir I pray you and to the world of it I am the king of mine I will go along and I will take my leave and I shall have no more than you have I rather not a man To do you do you see to me and you sir To be a traitor of your presence so and you shall bear the Tower I do not say I am ashamed to be a little fellow I am dead of you To me to you I shall find you sir To you that shall be brief and if you were a woman and I am knock to take him to the way but a good king And for my brother to your wife and the prince is gone with the other report of the feast Of the queens side O let me speak I am not still YORK I know not for my lord my lord I shall be consul To have you royally of my voices and I will serve you to see them And I am not think you to the matter with this sword With all the world of the selves that you are incensed to be a man of their face and my mother stand in their trade That you are in the world I will be seen to your honour and be rich and be gone to bed GLOUCESTER So now your honour is mine That say you to you and do you hear out to our friends And so of your own power I am sorry you have been in the field of this and recreant and you have been age As I shall bid the business And I will have no more a man that I have heard to see the duke be gentle men will hang him to your lordship and your wife and my daughter But in the feast of your own house and his hearts Where if you have no more LEONTES He has not all the palace of the time and this I know not my brother and my lord I am so much to be so and I can frame my lord I am in that he knows not a word I would have heard my lord to make a word of a most king A parasite a man have made me businesses the people or all the field of the world Provide his grace and that I am I say I would not say it is a kings king that I have heard to see you and I were not To be to your queen To tell you good the gods do you have been a man in a heart of victory I will not speak of thee and I am no a thing to be king KING RICHARD III And now to make thee gentle man And in the end of this land and this he hath been sworn in his own mouth and that I am gone I will be a king and let me see it is not for a good thing That we may have been so and in my master and the number that is a king Nurse Thourt my liege and all my mother and my lord I know not for my reasons sir I will not be gone with you to be a king And if you be youngest sir You have a maid of you for a very house and to make the scene of such a very apoplexy But in the king is done The gods of the house of the house of the field of the devil and this you are sure to be the house of the house of your own house And in the whole laws of the Tower and Edwards death Nor took his leave to make him then I will be brief and make a fearful thing To be a maim of this you being nothing and by my memory of my sight I will be whipt but you have been a poison to your worships body And that the father of the world which is a gross of me and I am not well I know your grace for my part I believe you to your body as I have seen you from the world O that I have been but a better man that you will find you so and to be so by a hands of your own house and I know this is a wife of your house and the rest of the kings here I will be revenged of the very bosom of the king and be gone to the Tower KING RICHARD III O I am most sir I have a little\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_str, gen_length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_seq = [dataset.char_to_int[c] for c in start_str[-dataset.seq_length:]]\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long).unsqueeze(0).to(model.device)\n",
    "\n",
    "    text = start_str\n",
    "    hidden = None\n",
    "\n",
    "    for _ in range(gen_length):\n",
    "        output, hidden = model.lstm(model.embedding(input_seq), hidden)\n",
    "        output_logits = model.fc(output[:, -1, :])\n",
    "        p = torch.nn.functional.softmax(output_logits / temperature, dim=-1).detach().cpu().numpy().squeeze()\n",
    "        char_ind = np.random.choice(len(dataset.chars), p=p)\n",
    "        next_char = dataset.int_to_char[char_ind]\n",
    "        text += next_char\n",
    "\n",
    "        input_seq = torch.cat((input_seq[:, 1:], torch.tensor([[char_ind]], dtype=torch.long).to(model.device)), dim=1)\n",
    "\n",
    "    return text\n",
    "\n",
    "    \n",
    "print(generate_text(model, 'ROMEO:', gen_length=1000, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rich extension is already loaded. To reload it, use:\n",
      "  %reload_ext rich\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from rich import print\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "%load_ext rich\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = os.path.join(os.getcwd(),\"data\")\n",
    "# path = \"/kaggle/input/20newsgroups/\"\n",
    "train = fetch_20newsgroups(data_home=path, subset=\"train\")\n",
    "test = fetch_20newsgroups(data_home=path, subset=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for model training\n",
    "config = {\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"MAX_LENGTH\": 512,\n",
    "    \"LEARNING_RATE\": 1e-5,\n",
    "    \"N_EMBED\": 768,\n",
    "    \"N_HEADS\": 2,\n",
    "    \"N_BLOCKS\": 12,\n",
    "    \"DROPOUT\": 0.2,\n",
    "    \"NUM_LABELS\": 20\n",
    "}\n",
    "\n",
    "# device = \"cuda\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tokenizer and Data Transformation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GPT2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# Set the pad token to the end of sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Add the pad token to the special tokens\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "\n",
    "\n",
    "# Define the Dataset class for the transformer model\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # Initialize the Dataset class\n",
    "    def __init__(self, data, tokenizer, max_length, target):\n",
    "        # Set the data, tokenizer, max_length, texts, and labels attributes\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = [text for text in data]  # Corrected attribute name from self.text to self.texts\n",
    "        self.labels = target\n",
    "\n",
    "    # Define the length method for the Dataset class\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Define the classes method for the Dataset class\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    # Define the getitem method for the Dataset class\n",
    "    def __getitem__(self, index):\n",
    "        # Tokenize the text at the given index\n",
    "        text_batch = tokenizer(\n",
    "            self.texts[index],\n",
    "            padding=\"max_length\",\n",
    "            max_length=config[\"MAX_LENGTH\"],\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Get the target label at the given index\n",
    "        target_batch = np.array(self.labels[index])\n",
    "        # Return the tokenized text and the target label\n",
    "        return text_batch, target_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_idx, val_idx = np.split(\n",
    "    np.random.permutation(len(train.data)), [int(0.8 * len(train.data))]\n",
    ")\n",
    "\n",
    "# Create datasets for training, validation, and testing\n",
    "# The Dataset class is defined earlier in the code\n",
    "train_dataset = Dataset([train.data[i] for i in train_idx], tokenizer, config['MAX_LENGTH'], train.target[train_idx])\n",
    "val_dataset = Dataset([train.data[i] for i in val_idx], tokenizer, config['MAX_LENGTH'], train.target[val_idx])\n",
    "test_dataset = Dataset(test.data, tokenizer, config['MAX_LENGTH'], test.target)\n",
    "\n",
    "# Create data loaders for training, validation, and testing\n",
    "# The DataLoader class is from PyTorch's torch.utils.data module\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['BATCH_SIZE'], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating components of transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class defines the attention layer in the transformer model.\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, head_size, config):\n",
    "        # Initialize the attention layer\n",
    "        super().__init__()\n",
    "        # Define the query, key, and value layers\n",
    "        self.query_layer = nn.Linear(config['N_EMBED'], head_size, bias=False)\n",
    "        self.key_layer = nn.Linear(config['N_EMBED'], head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(config['N_EMBED'], head_size, bias=False)\n",
    "        # Define the dropout layer\n",
    "        self.dropout = nn.Dropout(config['DROPOUT'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size, sequence length, and embedding size\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute the query, key, and value matrices\n",
    "        q = self.query_layer(x)\n",
    "        k = self.key_layer(x)\n",
    "        v = self.value_layer(x)\n",
    "\n",
    "        # Compute the attention weights\n",
    "        weights = q @ k.transpose(-2, -1) * (C**-0.5)\n",
    "\n",
    "        # Apply softmax and dropout to the attention weights\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # Compute the output\n",
    "        out = weights @ v\n",
    "        return out\n",
    "\n",
    "# This class defines the multi-head attention layer in the transformer model.\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size, config):\n",
    "        # Initialize the multi-head attention layer\n",
    "        super().__init__()\n",
    "        # Define the attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionLayer(head_size, config) for _ in range(n_heads)])\n",
    "        # Define the projection layer\n",
    "        self.proj = nn.Linear(config['N_EMBED'], config['N_EMBED'])\n",
    "        # Define the dropout layer\n",
    "        self.dropout = nn.Dropout(config['DROPOUT'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the output of each attention head\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        # Apply the projection layer\n",
    "        out = self.proj(out)\n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "# This class defines the feed forward layer in the transformer model.\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config, scale_factor=1):\n",
    "        # Initialize the feed forward layer\n",
    "        super().__init__()\n",
    "        # Define the feed forward network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config['N_EMBED'], scale_factor * config['N_EMBED']),\n",
    "            nn.GELU(),\n",
    "            # Projection layer\n",
    "            nn.Linear(scale_factor * config['N_EMBED'], config['N_EMBED']),\n",
    "            nn.Dropout(config['DROPOUT']),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the output of the feed forward network\n",
    "        return self.net(x)\n",
    "\n",
    "# This class defines the transformer block in the transformer model.\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        # Initialize the transformer block\n",
    "        super().__init__()\n",
    "        # Define the self-attention layer\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, n_embed // n_heads, config)\n",
    "        # Define the layer normalization layer\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "\n",
    "        # Define the feed forward layer\n",
    "        self.ffwd = FeedForward(n_embed, 4)\n",
    "        # Define the layer normalization layer\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the output of the self-attention layer\n",
    "        x = x + self.sa_heads(self.ln1(x))\n",
    "        # Compute the output of the feed forward layer\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class defines the transformer classifier model.\n",
    "class TransformerClassifier(L.LightningModule):\n",
    "    def __init__(self, hidden_size, num_classes, max_seq_len, n_heads, n_layers, lr=1e-5):\n",
    "        # Initialize the transformer classifier model\n",
    "        super().__init__()\n",
    "\n",
    "        # Load the GPT2 model configuration and model\n",
    "        self.gpt2config = GPT2Config.from_pretrained(\"gpt2\", n_layer=n_layers)\n",
    "        self.gpt2 = GPT2Model.from_pretrained(\"gpt2\", config=self.gpt2config)\n",
    "\n",
    "        # Initialize the transformer block\n",
    "        self.tf_block = TransformerBlock(hidden_size, n_heads)\n",
    "        # Initialize the classifier\n",
    "        self.classifier = nn.Linear(hidden_size * max_seq_len, num_classes)\n",
    "        # Set the learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Compute the output of the GPT2 model\n",
    "        gpt_out = self.gpt2(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, return_dict=True\n",
    "        ).last_hidden_state\n",
    "\n",
    "        # Pass through transformer block\n",
    "        gpt_out = self.tf_block(gpt_out)\n",
    "\n",
    "        # Flatten and pass through classifier layer\n",
    "        logits = self.classifier(gpt_out.view(gpt_out.size(0), -1))\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get the input and target\n",
    "        x, y = batch\n",
    "        input_ids = x[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = x[\"attention_mask\"].to(device)\n",
    "        y = y.to(device).long()\n",
    "\n",
    "        # Compute the logits\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        acc = (logits.argmax(1) == y).float().mean()\n",
    "\n",
    "        # Log the loss and accuracy\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get the input and target\n",
    "        x, y = batch\n",
    "        input_ids = x[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = x[\"attention_mask\"].to(device)\n",
    "        y = y.to(device).long()\n",
    "\n",
    "        # Compute the logits\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        acc = (logits.argmax(1) == y).float().mean()\n",
    "\n",
    "        # Log the loss and accuracy\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Print a random sample if it's the first batch\n",
    "        if batch_idx == 0:\n",
    "            r_idx = np.random.randint(0, len(y))  # Random index\n",
    "\n",
    "            print(\n",
    "                f\"Input: {tokenizer.decode(input_ids[r_idx], skip_special_tokens=True)}\"\n",
    "            )\n",
    "            print(f\"Label: {y[r_idx]}\")\n",
    "            print(f\"Prediction: {logits.argmax(1)[r_idx]}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Get the input and target\n",
    "        x, y = batch\n",
    "        input_ids = x[\"input_ids\"].squeeze(1).to(device)\n",
    "        attention_mask = x[\"attention_mask\"].to(device)\n",
    "        y = y.to(device).long()\n",
    "\n",
    "        # Compute the logits\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        acc = (logits.argmax(1) == y).float().mean()\n",
    "\n",
    "        # Log the loss and accuracy\n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Configure the optimizer\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create the transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is initialzed below. A trainer class was also created from PyTorch Lightning to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerClassifier(\n",
    "    config['N_EMBED'], config['NUM_LABELS'], config['MAX_LENGTH'], config['N_HEADS'], config['N_BLOCKS'], lr=config['LEARNING_RATE']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=2,\n",
    "    max_epochs=5,\n",
    "    callbacks=[ModelCheckpoint(monitor=\"val_acc\", mode=\"max\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | gpt2       | GPT2Model        | 124 M \n",
      "1 | tf_block   | TransformerBlock | 7.1 M \n",
      "2 | classifier | Linear           | 7.9 M \n",
      "------------------------------------------------\n",
      "139 M     Trainable params\n",
      "0         Non-trainable params\n",
      "139 M     Total params\n",
      "557.559   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5465da233c04a468e6124ec5fb612ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Input: Subject: **** Tapes <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> Sale <span style=\"font-weight: bold\">(</span>most sale<span style=\"font-weight: bold\">)</span> ****\n",
       "From: koutd@hirama.hiram.edu <span style=\"font-weight: bold\">(</span>DOUGLAS KOU<span style=\"font-weight: bold\">)</span>\n",
       "Organization: Hiram College\n",
       "Nntp-Posting-Host: hirama.hiram.edu\n",
       "Lines: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>\n",
       "\n",
       "Tapes for sale, $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.00</span> each and the shipping is included.\n",
       "Those tapes are <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> year old and are hardly used, so there should\n",
       "not be any problem with it. I really want to sell them, so make\n",
       "me a package offer if you wish to.\n",
       "\n",
       "Eagles          The Best of Eagles\n",
       "Eagles          Hotel California\n",
       "Elton John      Sleeping with the past\n",
       "Gloria Estefan  Into the Light\n",
       "James Ingram    The Power of Great Music\n",
       "Kenny G.        Duo Tones\n",
       "Lethal Weapon <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span> <span style=\"font-weight: bold\">(</span> music from the motion picture <span style=\"font-weight: bold\">)</span>\n",
       "Mariah Carey    MTV Unplugged EP\n",
       "Michael Bolton  Time, Love and Tenderness\n",
       "The Phantom of the Opera\n",
       "Genesis         We can't dance\n",
       "Phil Collins    <span style=\"color: #808000; text-decoration-color: #808000\">...</span> But Seriously\n",
       "Queen           The Works\n",
       "Queen           Live Magic\n",
       "Wilson Phillips\n",
       "\n",
       "Send me your offer<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "\n",
       "Please send your offer to koutd@hirama.hiram.edu\n",
       "\n",
       "thanks you,\n",
       "\n",
       "Douglas Kou\n",
       "Hiram College\n",
       "Hiram, Ohio\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Input: Subject: **** Tapes \u001b[1;36m4\u001b[0m Sale \u001b[1m(\u001b[0mmost sale\u001b[1m)\u001b[0m ****\n",
       "From: koutd@hirama.hiram.edu \u001b[1m(\u001b[0mDOUGLAS KOU\u001b[1m)\u001b[0m\n",
       "Organization: Hiram College\n",
       "Nntp-Posting-Host: hirama.hiram.edu\n",
       "Lines: \u001b[1;36m30\u001b[0m\n",
       "\n",
       "Tapes for sale, $\u001b[1;36m3.00\u001b[0m each and the shipping is included.\n",
       "Those tapes are \u001b[1;36m1\u001b[0m year old and are hardly used, so there should\n",
       "not be any problem with it. I really want to sell them, so make\n",
       "me a package offer if you wish to.\n",
       "\n",
       "Eagles          The Best of Eagles\n",
       "Eagles          Hotel California\n",
       "Elton John      Sleeping with the past\n",
       "Gloria Estefan  Into the Light\n",
       "James Ingram    The Power of Great Music\n",
       "Kenny G.        Duo Tones\n",
       "Lethal Weapon \u001b[1;36m3\u001b[0m \u001b[1m(\u001b[0m music from the motion picture \u001b[1m)\u001b[0m\n",
       "Mariah Carey    MTV Unplugged EP\n",
       "Michael Bolton  Time, Love and Tenderness\n",
       "The Phantom of the Opera\n",
       "Genesis         We can't dance\n",
       "Phil Collins    \u001b[33m...\u001b[0m But Seriously\n",
       "Queen           The Works\n",
       "Queen           Live Magic\n",
       "Wilson Phillips\n",
       "\n",
       "Send me your offer\u001b[33m...\u001b[0m\n",
       "\n",
       "Please send your offer to koutd@hirama.hiram.edu\n",
       "\n",
       "thanks you,\n",
       "\n",
       "Douglas Kou\n",
       "Hiram College\n",
       "Hiram, Ohio\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Label: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Label: \u001b[1;36m6\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Prediction: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Prediction: \u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b4d80f69c41509765b43c498527e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model training was conducted on Kaggle using GPU, but the model was trained using the same code specified in this notebook. The model weights were saved after trainning, which were then loaded in this notebook to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mAll\u001b[0m\u001b[39m keys matched successfully\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the TransformerClassifier model and move it to the device\n",
    "model = TransformerClassifier(\n",
    "    config['N_EMBED'], config['NUM_LABELS'], config['MAX_LENGTH'], config['N_HEADS'], config['N_BLOCKS'], lr=config['LEARNING_RATE']\n",
    ").to(device)\n",
    "\n",
    "# Load the state dictionary of the model\n",
    "# state_dict = torch.load(\"/kaggle/working/model/.ckpt\")\n",
    "state_dict = torch.load(\"./epoch=4-step=1415.ckpt\", map_location=device)\n",
    "model.load_state_dict(state_dict[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyy/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2179640f0f874550bcba73e2711a2bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yyy/Library/Python/3.9/lib/python/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the model on the test_loader\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the next batch of data from the test_loader\n",
    "X, y = next(iter(test_loader))\n",
    "\n",
    "# Get the logits for the first input in the batch\n",
    "logits = model(X[\"input_ids\"][1].to(device), X[\"attention_mask\"][1].to(device))\n",
    "\n",
    "# Print the decoded input, actual label and predicted label\n",
    "print(tokenizer.decode(X[\"input_ids\"][1][0], skip_special_tokens=True))\n",
    "print(f\"Actual Label: {y[1]}\")\n",
    "print(f\"Predicted Label: {logits.argmax(1).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTransformerClassifier\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mgpt2\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Model\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwte\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwpe\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mh\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m11\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m12\u001b[0m x \u001b[1;35mGPT2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mattn_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mresid_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mGPT2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mNewGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_f\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mtf_block\u001b[1m)\u001b[0m: \u001b[1;35mTransformerBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0msa_heads\u001b[1m)\u001b[0m: \u001b[1;35mMultiHeadAttention\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mheads\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m2\u001b[0m x \u001b[1;35mAttentionLayer\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mquery_layer\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mkey_layer\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mvalue_layer\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mffwd\u001b[1m)\u001b[0m: \u001b[1;35mFeedForward\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnet\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mclassifier\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m393216\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mTransformerClassifier\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mgpt2\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Model\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwte\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mwpe\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1024\u001b[0m, \u001b[1;36m768\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mdrop\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mh\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m11\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m12\u001b[0m x \u001b[1;35mGPT2Block\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mattn\u001b[1m)\u001b[0m: \u001b[1;35mGPT2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_attn\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mattn_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mresid_dropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mln_2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mGPT2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_fc\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mc_proj\u001b[1m)\u001b[0m: \u001b[1;35mConv1D\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mact\u001b[1m)\u001b[0m: \u001b[1;35mNewGELUActivation\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln_f\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mtf_block\u001b[1m)\u001b[0m: \u001b[1;35mTransformerBlock\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0msa_heads\u001b[1m)\u001b[0m: \u001b[1;35mMultiHeadAttention\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mheads\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m2\u001b[0m x \u001b[1;35mAttentionLayer\u001b[0m\u001b[1m(\u001b[0m\n",
       "          \u001b[1m(\u001b[0mquery_layer\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mkey_layer\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mvalue_layer\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m384\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "          \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mproj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln1\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mffwd\u001b[1m)\u001b[0m: \u001b[1;35mFeedForward\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[1m(\u001b[0mnet\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'none'\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m768\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0mln2\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m768\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mclassifier\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m393216\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m20\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
